version: '3'

services:
  fhir:
    depends_on:
      - mongo
    build:
      context: .
      dockerfile: Dockerfile
      args:
        NODE_ENV: development
#    env_file:
#      - .env
    #        mem_limit: 1g  # Set a memory limit of 1GB
    environment:
      SERVER_PORT: 3000
      MONGO_DB_NAME: fhir
      MONGO_URL: mongodb://mongo:27017
      AUDIT_EVENT_MONGO_DB_NAME: fhir
      AUDIT_EVENT_MONGO_URL: mongodb://mongo:27017
      RESOURCE_SERVER: http://localhost:3000/
      AUTH_SERVER_URI: http://myauthzserver.com
      CLIENT_ID: client
      CLIENT_SECRET: secret
      AUTH_CONFIGURATION_URI: https://cognito-idp.us-east-1.amazonaws.com/us-east-1_yV7wvD4xD/.well-known/openid-configuration
      AUTH_JWKS_URL: https://cognito-idp.us-east-1.amazonaws.com/us-east-1_yV7wvD4xD/.well-known/jwks.json
      AUTH_ISSUER: https://cognito-idp.us-east-1.amazonaws.com/us-east-1_yV7wvD4xD
      AUTH_CODE_FLOW_URL: https://bwell-dev.auth.us-east-1.amazoncognito.com
      AUTH_ENABLED: 1
      RENDER_HTML: 1
      REDIRECT_TO_LOGIN: 1
      MAX_INDEX_NAME_LENGTH: 65
      ENV: local
      MONGO_TIMEOUT: 30000
      LOGLEVEL: 'INFO'
      ENABLE_GRAPHQL: 1
      NODE_ENV: 'development'
      EXTERNAL_AUTH_JWKS_URLS: 'https://cognito-idp.us-east-1.amazonaws.com/us-east-1_e4iVm1J4X/.well-known/jwks.json'
      SET_INDEX_HINTS: 0
      CREATE_INDEX_ON_COLLECTION_CREATION: 1
      #      HASH_REFERENCE: 1
      RETURN_BUNDLE: '1'
      USE_TWO_STEP_SEARCH_OPTIMIZATION: '0'
      LOG_EXCLUDE_RESOURCES: 'Person,Patient'
      STREAM_RESPONSE: '1'
      ENABLE_MONGODB_ACCESS_LOGS: '1'
      LOG_STREAM_STEPS: '0'
      ENABLE_EVENTS_KAFKA: '1'
      ENABLE_KAFKA_HEALTHCHECK: '1'
      KAFKA_CLIENT_ID: 'fhir-server'
      KAFKA_URLS: 'kafka:9092'
      KAFKA_MAX_RETRY: 3
      PARTITION_RESOURCES: 'AuditEvent'
      AUTH_CUSTOM_GROUP: "cognito:groups"
      AUTH_CUSTOM_SCOPE: "custom:scope"
      VALIDATE_SCHEMA: "1"
      PERSON_MATCHING_SERVICE_URL: "https://person-matching.dev.bwell.zone/$$match"
      WHITELIST: "https://embeddable-sandbox.cdn.apollographql.com,http://localhost:5051"
      GRIDFS_RESOURCES: "DocumentReference"
      ENABLE_ACCESS_TAG_UPDATE: '0'
      DD_TRACE_ENABLED: 'False'
      ENABLE_GRAPHQL_PLAYGROUND: '1'
      ENABLE_CONSENTED_DATA_ACCESS: '1'
      FHIR_VALIDATION_URL: 'http://hapi-fhir-server:8080/fhir'
      OPENSEARCH_VECTORSTORE_URL: 'http://elasticsearch:9200'
      OPENSEARCH_VECTORSTORE_INDEX: 'fhir_summaries'
      OPENSEARCH_VECTORSTORE_USERNAME: 'admin'
      OPENSEARCH_VECTORSTORE_PASSWORD: 'admin'
      WRITE_FHIR_SUMMARY_TO_VECTORSTORE: '1'
      ENABLE_STATS_ENDPOINT: '1'
      FHIR_SERVER_UI_URL: 'http://localhost:5051'
#      MONGO_VECTORSTORE_URL: 'mongodb+srv://cl-dev-knowledge-pl-0.vpsmx.mongodb.net/?retryWrites=true&w=majority'
#      MONGO_VECTORSTORE_USERNAME: ''
#      MONGO_VECTORSTORE_PASSWORD: ''
#      MONGO_VECTORSTORE_DB: 'cl-dev-knowledge'
    ports:
      - '3000:3000'
    volumes:
      - ./src:/srv/src/src
      - ./package.json:/srv/src/package.json
      - ./yarn.lock:/srv/src/yarn.lock
    command: yarn run dev
    healthcheck:
      test: [ 'CMD-SHELL', 'curl --silent --fail localhost:3000/health || exit 1' ]

  mongo:
    image: mongo:5.0.22
    ports:
      - '27017:27017'
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
    volumes:
      - mongo_data:/data/db
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongo mongo:27017/test --quiet

  #  mongoclient:
  #    image: imranq2/mongoclient-aws:0.1.8
  #    ports:
  #      - '3010:3000'
  #    environment:
  #      MONGOCLIENT_DEFAULT_CONNECTION_URL: mongodb://mongo:27017

  zookeeper:
    image: 'docker.io/bitnami/zookeeper:3.9.1'
    ports:
      - '2181:2181'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    restart: on-failure
    healthcheck:
      test: [ 'CMD-SHELL', 'echo ruok | nc -w 2 zookeeper 2181' ]

  kafka:
    image: 'docker.io/bitnami/kafka:3.6.0'
    ports:
      - '9092:9092'
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKAJS_NO_PARTITIONER_WARNING=1
    depends_on:
      - zookeeper
    healthcheck:
      test:
        [
          'CMD',
          'bash',
          '-c',
          'unset',
          'JMX_PORT',
          ';',
          'kafka-topics.sh',
          '--zookeeper',
          'zookeeper:2181',
          '--list',
        ]

  kafkaUI:
    image: 'obsidiandynamics/kafdrop:4.0.1'
    platform: linux/x86_64
    ports:
      - '9000:9000'
    environment:
      - KAFKA_BROKERCONNECT=kafka:9092
      - 'JVM_OPTS=-Xms32M -Xmx64M'
      - SERVER_SERVLET_CONTEXTPATH=/

  hapi-fhir-server:
    image: hapiproject/hapi:v6.10.1
    ports:
      - "3001:8080"

  elasticsearch:
    #    image: bitnami/elasticsearch:7.9.3  # AWS Elastic Search supports upto 7.9
    # using OpenDistro for best compatibility with AWS Managed ElasticSearch:
    # https://opendistro.github.io/for-elasticsearch/downloads.html
    image: opensearchproject/opensearch:2.11.0
    container_name: elasticsearch
    environment:
      - cluster.name=odfe-cluster
      - node.name=elasticsearch
      - discovery.type=single-node
      #      - cluster.initial_master_nodes=elasticsearch
      - bootstrap.memory_lock=true # along with the memlock settings below, disables swapping
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m" # minimum and maximum Java heap size, recommend setting both to 50% of system RAM
      - network.host=0.0.0.0 # required if not using the demo security configuration
      #      - opendistro_security.disabled=true
      - cluster.routing.allocation.disk.threshold_enabled=false
      - plugins.security.ssl.http.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536 # maximum number of open files for the Elasticsearch user, set to at least 65536 on modern systems
        hard: 65536
    volumes:
      - es_data:/usr/share/opensearch/data:delegated
    #      - ./conf/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:cached
    ports:
      - '9200:9200'
      - '9600:9600' # required for Performance Analyzer
    healthcheck:
      test: [ "CMD-SHELL", "curl --silent --fail https://admin:admin@localhost:9200/_cluster/health --insecure || exit 1" ]

volumes:
  mongo_data:
  es_data:
  es_cert:
  prometheus_data: { }
  grafana_data: { }
