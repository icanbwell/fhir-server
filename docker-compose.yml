version: '3'

services:
    fhir:
        depends_on:
            - mongo
        build:
            context: .
            dockerfile: Dockerfile
            args:
                NODE_ENV: development
        environment:
            SERVER_PORT: 3000
            MONGO_DB_NAME: fhir
            MONGO_URL: mongodb://mongo:27017?replicaSet=rs0
            AUDIT_EVENT_MONGO_DB_NAME: fhir
            AUDIT_EVENT_MONGO_URL: mongodb://mongo:27017
            RESOURCE_SERVER: http://localhost:3000/
            AUTH_SERVER_URI: http://myauthzserver.com
            CLIENT_ID: client
            CLIENT_SECRET: secret
            AUTH_CONFIGURATION_URI: https://cognito-idp.us-east-1.amazonaws.com/us-east-1_yV7wvD4xD/.well-known/openid-configuration
            AUTH_JWKS_URL: https://cognito-idp.us-east-1.amazonaws.com/us-east-1_yV7wvD4xD/.well-known/jwks.json
            AUTH_ISSUER: https://cognito-idp.us-east-1.amazonaws.com/us-east-1_yV7wvD4xD
            AUTH_CODE_FLOW_URL: https://bwell-dev.auth.us-east-1.amazoncognito.com
            AUTH_ENABLED: 1
            CHECK_ACCESS_TAG_ON_SAVE: 1
            RENDER_HTML: 1
            REDIRECT_TO_LOGIN: 1
            MAX_INDEX_NAME_LENGTH: 65
            ENV: local
            MONGO_TIMEOUT: 30000
            LOGLEVEL: 'DEBUG'
            ENABLE_GRAPHQL: 1
            NODE_ENV: 'development'
            EXTERNAL_AUTH_JWKS_URLS: 'https://cognito-idp.us-east-1.amazonaws.com/us-east-1_e4iVm1J4X/.well-known/jwks.json'
            SET_INDEX_HINTS: 0
            CREATE_INDEX_ON_COLLECTION_CREATION: 1
            #      HASH_REFERENCE: 1
            RETURN_BUNDLE: '1'
            USE_TWO_STEP_SEARCH_OPTIMIZATION: '0'
            STREAM_RESPONSE: '1'
            LOG_ELASTIC_SEARCH_ENABLE: '1'
            ENABLE_MONGODB_ACCESS_LOGS: '0'
            ENABLE_MONGODB_ACCESS_LOGS_SEARCH: '0'
            ACCESS_LOGS_COLLECTION_NAME: 'access_logs'
            LOG_ELASTIC_SEARCH_URL: 'https://elasticsearch:9200/'
            ELASTIC_SEARCH_USERNAME: 'admin'
            ELASTIC_SEARCH_PASSWORD: 'admin'
            LOG_ELASTIC_SEARCH_PREFIX: 'fhir-logs'
            LOG_STREAM_STEPS: '0'
            ENABLE_EVENTS_KAFKA: '1'
            ENABLE_KAFKA_HEALTHCHECK: '1'
            KAFKA_CLIENT_ID: 'fhir-server'
            KAFKA_URLS: 'kafka:9092'
            PARTITION_RESOURCES: 'AuditEvent'
            AUTH_CUSTOM_GROUP: "cognito:groups"
            AUTH_CUSTOM_SCOPE: "custom:scope"
            VALIDATE_SCHEMA: "1"
            PERSON_MATCHING_SERVICE_URL: "https://person-matching.dev.bwell.zone/$$match"
            WHITELIST: "https://embeddable-sandbox.cdn.apollographql.com"
            GRIDFS_RESOURCES: "DocumentReference"
        ports:
            - '3000:3000'
        volumes:
            - ./src:/srv/src/src
            - ./package.json:/srv/src/package.json
            - ./yarn.lock:/srv/src/yarn.lock
        command: yarn run dev
        healthcheck:
            test: ['CMD-SHELL', 'curl --silent --fail localhost:3000/health || exit 1']

    mongo:
        image: mongo:5.0.14
        ports:
            - '27017:27017'
        environment:
            - ALLOW_EMPTY_PASSWORD=yes
        volumes:
            - mongo_data:/data/db
        command: "--replSet rs0"
        healthcheck:
            test: echo 'db.runCommand("ping").ok' | mongo mongo:27017/test --quiet

    #  mongoclient:
    #    image: imranq2/mongoclient-aws:0.1.8
    #    ports:
    #      - '3010:3000'
    #    environment:
    #      MONGOCLIENT_DEFAULT_CONNECTION_URL: mongodb://mongo:27017

    # The Mongo initialization server
    # this runs the `rs.initiate` command to intialize
    # the replica set
    mongoinit:
      image: mongo:5.0.14
      # this container will exit after executing the command
      restart: "no"
      depends_on:
        - mongo
      command:
        - bash
        - -c
        - |
          sleep 10 && mongo --host mongo:27017 --eval \
          '
          var config = {
              "_id": "rs0",
              "version": 1,
              "members": [
                  {
                      "_id": 1,
                      "host": "mongo:27017"
                  }
              ]
          };
          rs.initiate(config, {force: true});
          '

    elasticsearch:
        #    image: bitnami/elasticsearch:7.9.3  # AWS Elastic Search supports upto 7.9
        # using OpenDistro for best compatibility with AWS Managed ElasticSearch:
        # https://opendistro.github.io/for-elasticsearch/downloads.html
        image: opensearchproject/opensearch:1.3.8
        container_name: elasticsearch
        environment:
            - cluster.name=odfe-cluster
            - node.name=elasticsearch
            - bootstrap.memory_lock=true # along with the memlock settings below, disables swapping
            - network.host=0.0.0.0 # required if not using the demo security configuration
            - 'OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m' # minimum and maximum Java heap size, recommend setting both to 50% of system RAM
            #      - "DISABLE_INSTALL_DEMO_CONFIG=true" # disables execution of install_demo_configuration.sh bundled with security plugin, which installs demo certificates and security configurations to OpenSearch
            #      - "DISABLE_SECURITY_PLUGIN=true" # disables security plugin entirely in OpenSearch by setting plugins.security.disabled: true in opensearch.yml
            - 'discovery.type=single-node' # disables bootstrap checks that are enabled when network.host is set to a non-loopback address
        ulimits:
            memlock:
                soft: -1
                hard: -1
            nofile:
                soft: 65536 # maximum number of open files for the Elasticsearch user, set to at least 65536 on modern systems
                hard: 65536
        volumes:
            - es_data:/usr/share/opensearch/data:delegated
            - es_cert:/usr/share/opensearch/config/certs:delegated
        #      - ./conf/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:cached
        ports:
            - '9200:9200'
            - '9600:9600' # required for Performance Analyzer
        healthcheck:
            test:
                [
                    'CMD-SHELL',
                    'curl --silent --fail https://admin:admin@localhost:9200/_cluster/health --insecure || exit 1',
                ]

    kibana:
        #    image: bitnami/kibana:7.9.3 # AWS Elastic Search supports upto 7.9
        # using OpenDistro for best compatibility with AWS Managed ElasticSearch:
        # https://opendistro.github.io/for-elasticsearch/downloads.html
        image: opensearchproject/opensearch-dashboards:1.3.8
        depends_on:
            - elasticsearch
        container_name: kibana
        ports:
            - '5601:5601'
        environment:
            - OPENSEARCH_URL=https://elasticsearch:9200
            - OPENSEARCH_HOSTS=https://elasticsearch:9200
        #      - "DISABLE_SECURITY_DASHBOARDS_PLUGIN=true" # disables security dashboards plugin in OpenSearch Dashboards
        healthcheck:
            test: ['CMD-SHELL', 'curl --silent --fail localhost:5601/login || exit 1']

    zookeeper:
        image: 'docker.io/bitnami/zookeeper:3.8.0'
        ports:
            - '2181:2181'
        environment:
            - ALLOW_ANONYMOUS_LOGIN=yes
        restart: on-failure
        healthcheck:
            test: ['CMD-SHELL', 'echo ruok | nc -w 2 zookeeper 2181']

    kafka:
        image: 'docker.io/bitnami/kafka:3.2.1'
        ports:
            - '9092:9092'
        environment:
            - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
            - ALLOW_PLAINTEXT_LISTENER=yes
            - KAFKAJS_NO_PARTITIONER_WARNING=1
        depends_on:
            - zookeeper
        healthcheck:
            test:
                [
                    'CMD',
                    'bash',
                    '-c',
                    'unset',
                    'JMX_PORT',
                    ';',
                    'kafka-topics.sh',
                    '--zookeeper',
                    'zookeeper:2181',
                    '--list',
                ]

    schema-registry:
      image: confluentinc/cp-schema-registry:6.2.0
      container_name: schema-registry
      ports:
        - "8081:8081"
      depends_on:
        - kafka
      environment:
        SCHEMA_REGISTRY_HOST_NAME: schema-registry
        SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092

    kafka-connect:
        image: 'confluentinc/cp-server-connect-base:7.4.0'
        ports:
            - '8083:8083'
        environment:
          CONNECT_BOOTSTRAP_SERVERS: "kafka:9092"
          CONNECT_REST_PORT: 8083
          CONNECT_GROUP_ID: kafka-connect
          CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
          CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
          CONNECT_STATUS_STORAGE_TOPIC: _connect-status
          CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
          CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.storage.StringConverter
          CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
          CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
          CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
          CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
          CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
          CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
          #  ---------------
          CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components,/data/connect-jars
        depends_on:
            - kafka
            - schema-registry
        command:
          - bash
          - -c
          - |
            echo "Installing Connector"
            confluent-hub install mongodb/kafka-connect-mongodb:1.10.0
            #
            echo "Launching Kafka Connect worker"
            /etc/confluent/docker/run &
            #
            sleep infinity

    kafka-connect-init:
        image: 'confluentinc/cp-server-connect-base:7.4.0'
        depends_on:
            - kafka-connect
        command:
          - bash
          - -c
          - |
            sleep 90 && curl -X POST -H "Content-Type: application/json" \
            --data '
                { "name": "patient-source",
                  "config": {
                     "connector.class":"com.mongodb.kafka.connect.MongoSourceConnector",
                     "connection.uri":"mongodb://mongo:27017/",
                     "database":"fhir",
                     "collection":"Patient_4_0_0",
                     "pipeline":"[{\"$$match\": {\"$$and\": [{ \"operationType\": {\"$$in\": [\"insert\", \"update\", \"replace\"]} }, {}]} }]",
                     "topic.namespace.map": "{\"fhir\": \"fhir\"}",
                     "topic.seperator": "."
                  }
                }
                 ' http://kafka-connect:8083/connectors -w "\n"


    kafkaUI:
        image: 'obsidiandynamics/kafdrop:3.30.0'
        ports:
            - '9000:9000'
        environment:
            - KAFKA_BROKERCONNECT=kafka:9092
            - 'JVM_OPTS=-Xms32M -Xmx64M'
            - SERVER_SERVLET_CONTEXTPATH=/

volumes:
    mongo_data:
    es_data:
    es_cert:
    prometheus_data: {}
    grafana_data: {}
